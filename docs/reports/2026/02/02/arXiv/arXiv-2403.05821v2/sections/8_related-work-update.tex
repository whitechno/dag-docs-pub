

\section{Related Work}

Our optimizations build on recent work in LLM inference as well as prior work integrating machine learning and data management. We describe several major related areas below.

\vspace{-0.5em}
\noindent \textbf{Inference-optimized systems.} There has been a recent rise of dedicated systems for LLM inference, including FasterTransformer \cite{faster-transformers}, Orca \cite{orca-continous-batching}, vLLM \cite{vllm}, and SGLang \cite{sglang}. 
Many systems already explore developing memory-efficient GPU kernels that perform inference while leveraging shared prefixes. 
SGLang's RadixAttention \cite{sglang}, Hydragen \cite{hydragen}, and Cascade Inference \cite{cascade-inference} all implement optimized kernels. 
Our work builds upon prior work investigating high-throughput LLM inference and prefix caching for model serving. In addition, we leverage full workload information from batch queries to further improve performance in relational workloads.


% \shu{DBML}
\vspace{-0.5em}
\noindent \textbf{LLMs in Relational Data Analytics} 
Many systems support calling LLMs as operators on relational data, spanning from production database vendors like Databricks \cite{databricks-ai-functions}, Google BigQuery \cite{google-bigquery-llm} and AWS Redshift \cite{aws-redshift-llm} to programming frameworks like LOTUS \cite{lotus}. While these works provide APIs for running LLMs over relational data, they do not explore how reordering data can optimize KV cache hits. 
There is also a line of work ~\cite{noscope, prob-pred} that explores using cheaper models for approximate query generation. This orthogonal direction is not considered in our paper scope, as our work specifically focuses on calling LLMs as functions from inside a regular, given SQL query.

% NoScope~\cite{noscope}, BlazeIt~\cite{blazeit}, and Probabilistic Predicates~\cite{prob-pred} propose approximating expensive ML model calls with less expensive models for approximate query processing, but this can reduce query accuracy, and does not take advantage of the unique opportunities for KV cache reuse in LLM inference.
% However, past systems focus on the online setting and make no assumptions about the requests sent to the LLM. 

% \vspace{-0.5em}
% \noindent \textbf{Prefix Sharing.} Recent work explores developing memory-efficient GPU kernels that perform inference while leveraging shared prefixes. %to compute LLM attention leveraging shared prefix. 
% SGLang's RadixAttention \cite{sglang}, Hydragen \cite{hydragen}, and Cascade Inference \cite{cascade-inference} all implement optimized kernels. Our work heavily leverages these kernels to enable prefix sharing while delivering higher throughput as compared to traditional attention kernels \cite{flash-attention}. 
